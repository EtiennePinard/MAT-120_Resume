\section*{Chapitre 2: Matrices}

\subsection*{Définition}
\begin{itemize}
    \item[] Une matrice de type $m \times n$ sur un corp (dans notre cas sur $\mathbb{R}$ ou sur $\mathbb{C}$) est noté
          $A_{m \times n}$ et contient $m$ lignes et $n$ colonnes. \begin{equation*}
              A_{m \times n} = \begin{pmatrix}
                  a_{11} & a_{12}      & \dots  & a_{1n} \\
                  a_{21} & a_{22}      & \dots  & a_{2n} \\
                  \vdots & \phantom{a} & \ddots & \vdots \\
                  a_{m1} & a_{m2}      & \dots  & a_{mn}
              \end{pmatrix}
          \end{equation*}
    \item[] La matrice composé uniquement de 0 pour un certain type $m \times n$ est écrite $\mathbb{O}$
    \item[] $A_{m \times n}$ est une matrice carré si $m = n$.
    \item[] La matrice identité de type $n$ est écrite $I_n = I = \begin{pmatrix}
                  1      & 0           & \dots  & 0      \\
                  0      & 1           & \dots  & 0      \\
                  \vdots & \phantom{a} & \ddots & \vdots \\
                  0      & 0           & \dots  & 1
              \end{pmatrix}$
    \item[] Soit $A \in M_n(\mathbb{C})$. Alors on dit que $A$ est \begin{enumerate}[itemsep = 0.5em]
              \item triangulaire supérieur si $A = \begin{pmatrix}
                            a_{11} & a_{12}      & \dots  & a_{1n} \\
                            0      & a_{22}      & \dots  & a_{2n} \\
                            \vdots & \phantom{a} & \ddots & \vdots \\
                            0      & 0           & \dots  & a_{nn}
                        \end{pmatrix}$
              \item triangulaire inférieur si $A = \begin{pmatrix}
                            a_{11} & 0           & \dots  & 0      \\
                            a_{21} & a_{22}      & \dots  & 0      \\
                            \vdots & \phantom{a} & \ddots & \vdots \\
                            a_{n1} & a_{n2}      & \dots  & a_{nn}
                        \end{pmatrix}$
              \item triangulaire si $A$ est triangulaire supérieur ou triangulaire inférieur
              \item Diagonale si $A = \begin{pmatrix}
                            a_{11} & 0           & \dots  & 0      \\
                            0      & a_{22}      & \dots  & 0      \\
                            \vdots & \phantom{a} & \ddots & \vdots \\
                            0      & 0           & \dots  & a_{nn}
                        \end{pmatrix} = \text{diag}\{ a_{11}, a_{22}, \dots, a_{nn}\}$ \\[0.5em]
                    Noté que $I_n = \text{diag}\{ 1, 1, \dots, 1 \}$ ainsi que $\mathbb{O}_n = \text{diag}\{ 0, 0, \dots, 0 \}$
          \end{enumerate}
\end{itemize}

\subsection*{Opérations matricielles}
\begin{itemize}
    \item[] \textbf{Addition de matrices} \begin{itemize}
              \item[] L'addition de deux matrices $A$ et $B$ existe seulement si $A$ et $B$ sont du même type.
                    L'addition est commutatif, associatif, possède un élément neutre et possède un inverse.
          \end{itemize}
    \item[] \textbf{Multiplication par un scalaire} \begin{itemize}
              \item[] La multication par un scalaire existe toujours pour une matrice $A$. La multication
                    par un scalaire est associatif, distributif et commutatif.
          \end{itemize}
    \item[] \textbf{Produit matricielle} \begin{itemize}
              \item[] Soit $A = (a_{ij}) \in M_{m \times n}(\mathbb{C})$ et $B = (b_{ij}) \in M_{r \times s}(\mathbb{C})$. Alors
                    le produit matricielle $AB$ existe si $n = r$. Dans ce cas, on a \begin{equation*}
                        (AB)_{ij} = \sum_{s = 1}^{n} a_{is} b_{sj}
                    \end{equation*}
                    En gros, l'élément à la position $i, j$ de $AB$ est le produit scalaire entre la i-ème ligne
                    de A et la j-ème colonne de B.
          \end{itemize}
    \item[] \textbf{Propriétés du produit matricielle} \begin{itemize}
              \item[] Soit $\alpha, \beta$ des scalaires et $A, B, C$ des matrices dont le produit matricielle entre eux existe. Alors \begin{enumerate}
                        \item $(AB)C = A(BC)$, produit matricielle est associatif
                        \item $AB \neq BA$, généralement, le produit matricielle n'est pas commutatif
                        \item $A(B + C) = AB + AC$, distributivité par la gauche
                        \item $(A + B)C = AC + BC$, distributivité par la droite
                        \item $AI = IA = A$, l'identité est l'élément neutre du produit matricielle
                    \end{enumerate}
          \end{itemize}
    \item[] \textbf{Transposition} \begin{itemize}
              \item[] La transposé de $A = (a_{ij})_{m \times n}$ est $A^{T} = (a_{ji})_{n \times m}$.
                    Pour une matrice carré, c'est une rotation des anti-diagonale par rapport à la grande diagonale.
          \end{itemize}
    \item[] \textbf{Conjugée hermitien} \begin{itemize}
              \item[] Le conjugué hermitien de $A = (a_{ij})_{m \times n}$ est $A^{\dagger} = \left(A^T\right)^\star = \left(A^\star\right)^T = (a_{ji}^\star)_{n \times m}$.
                    Le conjugué hermitien est aussi appellé la transposé conjugué.
          \end{itemize}
    \item[] \textbf{Propriétés transposé et conjugué hermitien} \begin{align*}
               & \left( A^T \right)^T = A               &  & \left( A^\dagger \right)^\dagger = A                     \\
               & \left( \alpha A \right)^T = \alpha A^T &  & \left( \alpha A \right)^\dagger = \alpha^\star A^\dagger \\
               & \left( A + B \right)^T = A^T + B^T     &  & \left( A + B \right)^\dagger = A^\dagger + B^\dagger     \\
               & \left( AB \right)^T = B^TA^T           &  & \left( AB \right)^\dagger = B^\dagger A^\dagger
          \end{align*}
    \item[] \textbf{Définition} \begin{enumerate}
              \item Si $A^T = A$, alors $A$ est dite symétrique. Si $A^T = -A$, alors $A$ est dite anti-symétrique.
              \item Si $A$ est une matrice carré tel que $AA^T = A^TA = I$, alors $A$ est appellé orthogonal.
              \item Si $A^\dagger = A$, alors $A$ est dite hermitienne. Si $A^\dagger = -A$, alors $A$ est dite anti-hermitienne.
              \item Si $A$ est une matrice carré tel que $AA^\dagger = A^\dagger A = I$, alors $A$ est appellé unitaire.
          \end{enumerate}
    \item[] \textbf{Commutateur de matrice} \begin{itemize}
              \item[] Soit $A, B \in M_n(\mathbb{C})$, alors le commutateur de $A$ et $B$ est la matrice \begin{equation*}
                        \left[ A, B \right] = AB - BA
                    \end{equation*}
          \end{itemize}
    \item[] \textbf{Propriétés du commutateur} \begin{enumerate}
              \item $[A, B] = -[B, A]$
              \item $[\alpha A, B ] = \alpha [A, B] $
              \item $[A + B, C] = [A, C] + [B, C]$
              \item $[A, B]^T = -[A^T, B^T]$
              \item $[[A, B], C] + [[C, A], B] + [[B, C], A] = \mathbb{O}$, l'identité de Jacobi
          \end{enumerate}
    \item[] \textbf{Trace d'une matrice} \begin{itemize}
              \item[] Soit $A = (a_{ij})_{n \times n}$. Alors la trace de $A$ est \begin{equation*}
                        \text{tr} A =  \sum_{i = 0}^{n} a_{ii}
                    \end{equation*}
          \end{itemize}
    \item[] \textbf{Propriétés de la trace} \begin{enumerate}
              \item $\text{tr}\{ AB \} = \text{tr}\{ BA \}$
              \item $\text{tr}\{ A + B \} = \text{tr}A + \text{tr}B$
              \item $\text{tr}\{ \alpha A \} = \alpha \text{tr}A $
              \item $\text{tr}\{ A^T \} = \text{tr}A$
              \item $\text{tr}\{ [A, B] \} = \text{tr}\{ AB - BA \} = \text{tr}\{ AB \} - \text{tr}\{BA\} = \text{tr}\{ AB \} - \text{tr}\{AB\} = 0$
          \end{enumerate}
\end{itemize}

\subsection*{Déterminant d'une matrice carré}
\begin{itemize}
    \item[] \textbf{Défintion du déterminant par récurrence} \begin{itemize}
              \item[] Le déterminant de $A \in M_n(\mathbb{C})$ noté $\det A = |A|$ est une nombre complexe
                    qui peut-être définie par récurrence sur n comme suit \begin{equation*}
                        \det A = \begin{cases}
                            a_{11},                                   & \text{si } n = 1 \\
                            \sum_{i = 1}^{n} (-1)^{j + i}a_{ji}m_{ji} & \text{si } n > 1
                        \end{cases}
                    \end{equation*}
              \item[] Ceci est l'expansion par la jème ligne, $1 \leq j \leq n$. Le terme $m_{ji}$
                    est le déterminant de la sous-matrice qu'on obtient si on biffe la ligne $j$ et la
                    colonne $i$. Si $M_{ji}$ est la sous-matrice obtenue en biffant la ligne $j$ et
                    la colonne $i$ alors $m_{ji} = \det M_{ji}$.
              \item[] Dans la première équation, le déterminant des sous-matrices était multiplié par la jème ligne. Il est
                    équivalent de développer le déterminant suivant la jème colonne, ce qui donne cette équation \begin{equation*}
                        \det A = \begin{cases}
                            a_{11},                                   & \text{si } n = 1 \\
                            \sum_{i = 1}^{n} (-1)^{i + j}a_{ij}m_{ij} & \text{si } n > 1
                        \end{cases}
                    \end{equation*}
          \end{itemize}
    \item[] \textbf{Défintion du déterminant par les permutations} \begin{itemize}
              \item[] Le déterminant de $A$ peut aussi être définie de cette façons \begin{equation*}
                        \det A = \sum_{\sigma \in S_n} \text{sign}(\sigma) \prod_{i = 1}^{n}a_{j \sigma_j}
                    \end{equation*}
                    Ici $\sigma = \begin{pmatrix}
                            \sigma_1 & \sigma_2 & \dots & \sigma_n
                        \end{pmatrix}$ et représente une permutation de l'ensemble $\{1, \ 2, \dots, \ n  \}$.
                    $\sigma_j$ est le jème élément de cette permutation. $S_n$ est l'ensemble de tous ces
                    permutations. $\text{sign}(\sigma)$ est la signature de $\sigma$ et est égale à
                    $(-1)^{\text{nb de désorde de } \sigma}$. Le désorde de $\sigma$ est le nombre de couple
                    $(\sigma_j, \sigma_k)$ ou $j < k$ mais $\sigma_j > \sigma_k$.
          \end{itemize}
    \item[] \textbf{Propriétés du déterminant} \begin{itemize}
              \item[] Soit $A, B \in M_n(\mathbb{C}), \ \alpha \in \mathbb{C}$ \begin{enumerate}
                        \item $\det A^T = \det A$
                        \item $\det(\alpha A) = \alpha^n \det(A)$
                        \item $\det (A^\star) = (\det A)^\star$
                        \item $\det A^\dagger = (\det A)^\star$
                        \item $\det(AB) = \det(A) \det(B)$
                        \item $\det(A^m) = (\det A)^m$
                        \item $\det(A + B) \neq \det A + \det B$
                        \item $\det(\text{diag}\{a_1, \ a_2, \ldots, a_n \}) = a_1 a_2 \dots a_n$
                        \item Si $A$ est triangulaire alors $\det A = a_{11} a_{22} \dots a_{nn}$
                    \end{enumerate}
              \item[] Soit $A = \begin{pmatrix}
                            L_1    \\
                            L_2    \\
                            \vdots \\
                            L_n
                        \end{pmatrix} = \arraycolsep=0.3\arraycolsep \begin{pmatrix}
                            C_1 & C_2 & \ldots & C_n
                        \end{pmatrix}$ ou $L_j \in M_{1 \times n}(\mathbb{C}), \ C_j \in M_{n\times 1}(\mathbb{C})$ \begin{enumerate}[itemsep = 0.5em]
                        \item $\det \begin{pmatrix}
                                      L_1   \\
                                      \dots \\
                                      L_j   \\
                                      \dots \\
                                      L_k   \\
                                      \dots \\
                                      L_n
                                  \end{pmatrix} = -\det \begin{pmatrix}
                                      L_1   \\
                                      \dots \\
                                      L_k   \\
                                      \dots \\
                                      L_j   \\
                                      \dots \\
                                      L_n
                                  \end{pmatrix} \\[0.5em] \det \arraycolsep=0.3\arraycolsep \begin{pmatrix}
                                      C_1 & \ldots & C_j & \ldots & C_k & \ldots & C_n
                                  \end{pmatrix} = -\det \begin{pmatrix}
                                      C_1 & \ldots & C_k & \ldots & C_j & \ldots & C_n
                                  \end{pmatrix}$
                        \item $\det \begin{pmatrix}
                                      L_1   \\
                                      \dots \\
                                      L_j   \\
                                      \dots \\
                                      L_j   \\
                                      \dots \\
                                      L_n
                                  \end{pmatrix} = 0 \\[0.5em] \det \arraycolsep=0.3\arraycolsep \begin{pmatrix}
                                      C_1 & \ldots & C_j & \ldots & C_j & \ldots & C_n
                                  \end{pmatrix} = 0$
                        \item $\det \begin{pmatrix}
                                      L_1        \\
                                      \dots      \\
                                      \alpha L_j \\
                                      \dots      \\
                                      L_n
                                  \end{pmatrix} = \alpha \det \begin{pmatrix}
                                      L_1   \\
                                      \dots \\
                                      L_j   \\
                                      \dots \\
                                      L_n
                                  \end{pmatrix} \\[0.5em] \det \arraycolsep=0.3\arraycolsep \begin{pmatrix}
                                      C_1 & \ldots & \alpha C_j & \ldots & C_n
                                  \end{pmatrix} = \alpha \det \begin{pmatrix}
                                      C_1 & \ldots & C_j & \ldots & C_n
                                  \end{pmatrix}$
                        \item $\det \begin{pmatrix}
                                      L_1   \\
                                      \dots \\
                                      L_j   \\
                                      \dots \\
                                      L_k   \\
                                      \dots \\
                                      L_n
                                  \end{pmatrix} = \det \begin{pmatrix}
                                      L_1              \\
                                      \dots            \\
                                      L_j + \alpha L_k \\
                                      \dots            \\
                                      L_k              \\
                                      \dots            \\
                                      L_n
                                  \end{pmatrix} \\[0.5em] \det \arraycolsep=0.3\arraycolsep \begin{pmatrix}
                                      C_1 & \ldots & C_j & \ldots & C_k & \ldots & C_n
                                  \end{pmatrix} = \det \begin{pmatrix}
                                      C_1 & \ldots & C_j + \alpha C_k & \ldots & C_k & \ldots & C_n
                                  \end{pmatrix}$
                        \item $\det \begin{pmatrix}
                                      L_1   \\
                                      \dots \\
                                      M + N \\
                                      \dots \\
                                      L_n
                                  \end{pmatrix} = \det \begin{pmatrix}
                                      L_1   \\
                                      \dots \\
                                      M     \\
                                      \dots \\
                                      L_n
                                  \end{pmatrix} + \det \begin{pmatrix}
                                      L_1   \\
                                      \dots \\
                                      N     \\
                                      \dots \\
                                      L_n
                                  \end{pmatrix}  \\[0.5em] \det \arraycolsep=0.3\arraycolsep \begin{pmatrix}
                                      C_1 & \ldots M + N & \ldots & C_n
                                  \end{pmatrix} = \det \begin{pmatrix}
                                      C_1 & \ldots M & \ldots & C_n
                                  \end{pmatrix} + \det \begin{pmatrix}
                                      C_1 & \ldots N & \ldots & C_n
                                  \end{pmatrix}$
                    \end{enumerate}
          \end{itemize}
\end{itemize}

% Temp break for current formatting. Will change if formatting changes
\break
\subsection*{Matrice inverses}
\begin{itemize}
    \item[] $A \in M_{n \times n} \text{ est inversible} \iff \exists B \text{ t.q } AB = BA = I$. Alors $B$ est noté $A^{-1}$
    \item[] \textbf{Propriétés de l'inverse} \begin{itemize}
              \item[] Soit $A \in M_{n \times n}(\mathbb{C})$ inversible \begin{enumerate}
                        \item $AB = \mathbb{O} \implies B = \mathbb{O}$
                        \item Si $AC = BA = I \implies B = BI = B(AC) = (BA)C = IC = C $, l'inverse à gauche et à droite sont égaux
                        \item $(A^{-1})^{-1} = A$, \quad $(A^T)^{-1} = (A^{-1})^{T}$, \quad $(A^\dagger)^{-1} = (A^{-1})^{\dagger}$
                        \item $(AB)^{-1} = B^{-1}A^{-1}$
                    \end{enumerate}
          \end{itemize}
    \item[] \textbf{Calculer la matrice inverse}
          \begin{itemize}
              \item[] Soit $A \in M_{n \times n}(\mathbb{C})$, alors
                    \begin{equation*}
                        A^{-1} = \frac{1}{\det A} \text{ adj}(A)
                    \end{equation*}
              \item[] $\text{adj}(A)$ est la matrice adjointe de $A$ et est définie comme \begin{equation*}
                        \text{adj}(A) = \left( \left( c_{ij} \right)_{n \times n} \right)^T
                    \end{equation*}
              \item[] $c_{ij}$ est le cofacteur d'incice $ij$ de $A$ et est défini comme \begin{equation*}
                        c_{ij} = (-1)^{i + j}m_{ij}
                    \end{equation*}
              \item[] $m_{ij}$ est le déterminant de la sous-matrice obtenue en biffant la ligne $i$ et la colonne $j$.
              \item[] On peut donc remarquer que $\det A = \sum_{k = 1}^{n} = a_{jk} c_{jk}$
              \item[] On a donc ce résultat pour les matrices inverses, \begin{equation*}
                        A \text{ est inversible} \iff \det A \neq 0
                    \end{equation*}
          \end{itemize}
\end{itemize}

\subsection*{Systèmes d'équations linéaires}
\begin{itemize}
    \item[] Chaque système d'équation linéaire peut-être représenter par une equation matricielle, soit \begin{equation*}
              \begin{cases}
                  a_{11}x_1 + a_{12}x_2 + \ldots + a_{1n}x_n & = b_1 \\
                  a_{21}x_1 + a_{22}x_2 + \ldots + a_{2n}x_n & = b_2 \\
                  \vdots                                             \\
                  a_{m1}x_1 + a_{m2}x_2 + \ldots + a_{mn}x_n & = b_m
              \end{cases} \iff \begin{pmatrix}
                  a_{11} & a_{12}      & \dots  & a_{1n} \\
                  a_{21} & a_{22}      & \dots  & a_{2n} \\
                  \vdots & \phantom{a} & \ddots & \vdots \\
                  a_{m1} & a_{m2}      & \dots  & a_{mn}
              \end{pmatrix} \begin{pmatrix}
                  x_1    \\
                  x_2    \\
                  \vdots \\
                  x_n
              \end{pmatrix} = \begin{pmatrix}
                  b_1    \\
                  b_2    \\
                  \vdots \\
                  b_m
              \end{pmatrix}
              \end{equation*}
    \item[] Le système devient $AX = B \iff (A|B)$. $(A|B)$ est dit $A$ augmenté de $B$ et est la matrice augmenté du système.
    \item[] \textbf{Forme échelonné d'un système} \begin{itemize}
              \item[] Une matrice est de la forme échelonné si \begin{enumerate}
                        \item La ligne qui précède une ligne non-nulle est non-nulle
                        \item Le premier coefficient non-nul d'une ligne non-nulle, appellé le pivot, est plus à
                              gauche que le premier coefficient non-nul de la ligne suivante.
                    \end{enumerate}
              \item[] On peut simplifier le système si on obtient la forme échelonné de la matrice augmenté. On obtient la forme échelonné
                    en appliquer des opérations élémentaires sur les lignes de $A$ et $B$. Ces opérations sont
                    échanger deux lignes, $L_j \leftrightarrow L_k$, additionner le multiple d'une ligne à une autre, 
                    $L_j \mapsto L_j + \alpha L_k$ et multiplier une ligne par un scalaire non-nul, $L_j \mapsto \alpha L_j, \ \alpha \neq 0$.
          \end{itemize}
    \item[] \textbf{Rang d'une matrice} \begin{itemize}
              \item[] $\text{rg}(A)$ est le rang de $A$ et est le nombre de ligne non-nulle de la forme échelonné
                    de $A$. On a alors trois cas pour un système linéaire $AX = B$, soit \begin{enumerate}
                        \item Si $\text{rg}(A) < rg(A|B)$, alors le système n'a pas de solution, il est incompatible
                        \item Si $\text{rg}(A) = rg(A|B) = n$, alors le système a une unique solution, il est compatible
                        \item Si $\text{rg}(A) = rg(A|B) < n$, alors le système a une infinité de solution, il est compatible
                    \end{enumerate}
              \item[] Dans le dernier cas, le système a une infinité de solution puisque qu'il a des variables libres,
                    soit exactement $n - \text{rg}(A)$ variables libres.
              \item[] Remarquons que si $A^\prime$ est la forme échelonné de $A$, alors $\det A = \alpha \det A^\prime, \ \alpha \in \mathbb{C}$.
                    Cela nous permet d'arriver au résultat \begin{equation*}
                        \det A \neq 0 \iff \text{rg}(A) = n \iff A \text{ est inversible} \iff AX=B \text{ a une unique solution}
                    \end{equation*}
              \item[] L'unique solution dans ce cas est $X = A^{-1}B$, puisque $A$ est inversible.
          \end{itemize}
    \item[] \textbf{Calculer l'inverse d'une matrice} \begin{itemize}
              \item[] Il est possible de calculer l'inverse de $A$ en échelonnant le système $(A|I)$. Le
                    système échelonné va donner $(A|I) \sim (I|B)$, avec $B = A^{-1}$.
          \end{itemize}
    \item[] \textbf{Système homogène} \begin{itemize}
              \item[] Un système homogène est un système de la forme $AX = \mathbb{O}$. Un tel système
                    est toujours compatible avec la solution trivial $X = \left(\begin{smallmatrix}
                                0 \\
                                \ldots \\
                                0
                            \end{smallmatrix}\right)$. De plus, si $A$ est inversible le système
                    homogène à seulement la solution trivial, sinon il a une infinité de
                    solution, puisque c'est impossible que $\text{rg}(A) < \text{rg}(A|\left(\begin{smallmatrix}
                                0 \\
                                \ldots \\
                                0
                            \end{smallmatrix}\right))$, et donc que le système soit incompatible.
          \end{itemize}
    \item[] \textbf{Noyau d'un système} \begin{itemize}
              \item[] Soit $A \in M_{m \times n}(\mathbb{C})$. Le noyau de $A$, noté $\text{N}(A)$, est l'ensemble de tous les solutions du système homogène 
            $AX = \mathbb{O}$, soit \begin{equation*}
                        \text{N}(A) = \left\{ X \in M_{n \times 1}(\mathbb{C}) \ | \ AX = \mathbb{O}  \right\}
                    \end{equation*}
          \end{itemize}
\end{itemize}